{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cnn_vae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harvey2phase/rrh-MNIST/blob/main/cnn_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlBt0FWz8iXu"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JmIrFw-0GXL"
      },
      "source": [
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFpBblS_eLg1"
      },
      "source": [
        "GPU = True\n",
        "device = torch.device(\"cuda:0\" if GPU and torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfwhdEx46mFx"
      },
      "source": [
        "# Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8UxKtJy51hY"
      },
      "source": [
        "MAX_ROT_DEG = 15\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9VbzfzV47ON"
      },
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "rot_transform = transforms.RandomRotation(MAX_ROT_DEG)\n",
        "\n",
        "def load_mnist(train, batch_size = BATCH_SIZE):\n",
        "    dataset = MNIST(\n",
        "        root = './data/MNIST',\n",
        "        download = True,\n",
        "        train = train,\n",
        "        transform = img_transform,\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Re9M0QYHy8"
      },
      "source": [
        "train_dataloader = load_mnist(train = True)\n",
        "test_dataloader = load_mnist(train = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMlt_q67tJo"
      },
      "source": [
        "# Place into numpy arrays for easier manipulation\n",
        "traindata = list(train_dataloader)\n",
        "traindata = [[sample[0].numpy(), sample[1].numpy()] for sample in traindata]\n",
        "train_X = np.vstack([sample[0] for sample in traindata])\n",
        "train_y = np.hstack([sample[1] for sample in traindata])\n",
        "\n",
        "testdata = list(test_dataloader)\n",
        "testdata = [[sample[0].numpy(), sample[1].numpy()] for sample in testdata]\n",
        "test_X = np.vstack([sample[0] for sample in testdata])\n",
        "test_y = np.hstack([sample[1] for sample in testdata])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugFXXKS40L21"
      },
      "source": [
        "# Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNN5g5cThMYp"
      },
      "source": [
        "def freeze(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMvoDB4Pz-fN"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQxQ4mV866aL"
      },
      "source": [
        "CNN_TRAIN_BATCH_SIZE = 64\n",
        "CNN_TEST_BATCH_SIZE = 1000\n",
        "CNN_EPOCHS = 14\n",
        "LR = 1.0\n",
        "GAMMA = 0.7\n",
        "LOG_INT = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSlDXhl-77_a"
      },
      "source": [
        "cnn_train_dataloader = load_mnist(True, batch_size = CNN_TRAIN_BATCH_SIZE)\n",
        "cnn_test_dataloader = load_mnist(False, batch_size = CNN_TEST_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw9LQeR8z__y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layers(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "    def hidden_layers(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(\n",
        "    model, optimizer,\n",
        "    device = device, train_loader = cnn_train_dataloader, epoch = 14\n",
        "):\n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #if batch_idx % LOG_INT == 0:\n",
        "        if False:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "def test(model, device = device, test_loader = cnn_test_dataloader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  \n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim = 1, keepdim = True)  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltsr1tE-3o4Z"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_tg2DAD7U3w"
      },
      "source": [
        "cnn = Net().to(device)\n",
        "optimizer = torch.optim.Adadelta(cnn.parameters(), lr=LR)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, CNN_EPOCHS + 1):\n",
        "    train(cnn, optimizer)\n",
        "    test(cnn)\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wITtuk2hkdfa"
      },
      "source": [
        "freeze(cnn)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bi1oJIrfksN"
      },
      "source": [
        "### TODO: figure out saving and loading models on Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAg4fEcS9156"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoZjwrH3xpei"
      },
      "source": [
        "#SEED = 100\n",
        "\n",
        "LAT_DIM = 2\n",
        "EPOCH_NUM = 50\n",
        "CAPACITY = 64\n",
        "LRN_RATE = 1e-3\n",
        "VAR_BETA = 1\n",
        "\n",
        "KERN_SIZE = 4\n",
        "STRIDE = 2\n",
        "PAD = 1\n",
        "\n",
        "RECON_CH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCQPtw5a3DQI"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.fc_mu = nn.Linear(\n",
        "            in_features = 10,\n",
        "            out_features = LAT_DIM,\n",
        "        )\n",
        "        self.fc_logvar = nn.Linear(\n",
        "            in_features = 10,\n",
        "            out_features = LAT_DIM,\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.fc = nn.Linear(\n",
        "            in_features = LAT_DIM,\n",
        "            out_features = 10,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return torch.sigmoid(\n",
        "            x.view(x.size(0), 10)\n",
        "        )\n",
        "\n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "\n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CrBYQU4e8r"
      },
      "source": [
        "def reconstruction_error(recon_x, x):\n",
        "    return F.binary_cross_entropy(\n",
        "        recon_x.view(-1, 1280),\n",
        "        x.view(-1, 1280),\n",
        "        reduction = \"sum\",\n",
        "    )\n",
        "\n",
        "def vae_loss(recon_loss, mu, logvar):\n",
        "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + VAR_BETA * kl_divergence"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86cSR8lZ4AK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "44f5213f-e659-4702-dc6f-7654134ea43b"
      },
      "source": [
        "vae1 = train_one_model()\n",
        "vae2 = train_one_model()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 74\n",
            "Training: \n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([128, 10])\n",
            "2\n",
            "torch.Size([96, 10])\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-dddc9d0f330d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvae2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-a45d4e49c712>\u001b[0m in \u001b[0;36mtrain_one_model\u001b[0;34m(evaluate)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# reconstruction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-c4c1f2848a71>\u001b[0m in \u001b[0;36mreconstruction_error\u001b[0;34m(recon_x, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     return F.binary_cross_entropy(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mrecon_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1280\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1280]' is invalid for input of size 960"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7XQuBW63DmN"
      },
      "source": [
        "### Training and Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FVd05s4I4YM"
      },
      "source": [
        "def plot_loss(train_losses, test_losses):\n",
        "    plt.ion()\n",
        "\n",
        "    plotlabels = [\"Total error\", \"Reconstruction error\"]\n",
        "    \n",
        "    ncols = 2\n",
        "    fig, ax = plt.subplots(ncols = ncols, figsize = (9, 2.5))\n",
        "    \n",
        "    for i in range(2): \n",
        "        ax[i].plot(train_losses[i], c = \"blue\", label = \"training\")\n",
        "        ax[i].plot(test_losses[i], c = \"red\", label = \"test\")\n",
        "            \n",
        "        ax[i].set_title(plotlabels[i])\n",
        "        \n",
        "    plt.xlabel('Epochs')\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel(y)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb5IpN6M3KKk"
      },
      "source": [
        "def train_one_model(evaluate = False):\n",
        "    \"\"\" Creates and trains one VAE model.\n",
        "\n",
        "    Args:\n",
        "        evaluate: Whether or not to evaluate model during training.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing the trained model, average training reconstruction\n",
        "        error, average total training error, average testing reconstruction\n",
        "        error, and average total testing error.\n",
        "    \"\"\"\n",
        "\n",
        "    #torch.cuda.manual_seed(SEED)\n",
        "    vae = VariationalAutoencoder()\n",
        "    vae = vae.to(device)\n",
        "    \n",
        "    num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "    print('Number of parameters: %d' % num_params)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(\n",
        "        params = vae.parameters(),\n",
        "        lr = LRN_RATE,\n",
        "        weight_decay = 1e-5,\n",
        "    )\n",
        "    \n",
        "    # set to training mode\n",
        "    vae.train()\n",
        "    \n",
        "    if evaluate:\n",
        "        train_recon_loss, train_loss = [], []\n",
        "        test_recon_loss, test_loss = [], []\n",
        "    \n",
        "    print(\"Training: \", end = \"\")\n",
        "    print()\n",
        "    for epoch in range(EPOCH_NUM):\n",
        "        if evaluate:\n",
        "            train_loss.append(0)\n",
        "            train_recon_loss.append(0)\n",
        "        num_batches = 0\n",
        "        \n",
        "        for image_batch, _ in train_dataloader:\n",
        "            \n",
        "            #image_batch = rot_transform(image_batch)\n",
        "            image_batch = image_batch.to(device)\n",
        "            image_batch = cnn.hidden_layers(image_batch)\n",
        "    \n",
        "            # vae reconstruction\n",
        "            print(image_batch.shape)\n",
        "            image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "            print(2)\n",
        "            \n",
        "            # reconstruction error\n",
        "            recon_loss = reconstruction_error(image_batch_recon, image_batch)\n",
        "            loss = vae_loss(recon_loss, latent_mu, latent_logvar)\n",
        "            \n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # one step of the optmizer (using the gradients from backpropagation)\n",
        "            optimizer.step()\n",
        "            \n",
        "            if evaluate:\n",
        "                train_loss[-1] += loss.item()\n",
        "                train_recon_loss[-1] += recon_loss\n",
        "                \n",
        "            num_batches += 1\n",
        "            \n",
        "        if evaluate:\n",
        "            train_loss[-1] /= num_batches\n",
        "            train_recon_loss[-1] /= num_batches\n",
        "        \n",
        "            recon_loss_avg, loss_avg = eval_model(vae)\n",
        "            test_recon_loss.append(recon_loss_avg)\n",
        "            test_loss.append(loss_avg)\n",
        "            vae.train()\n",
        "        \n",
        "        print(\"%d, \" % (epoch+1), end = \"\")\n",
        "        \n",
        "    print()\n",
        "    if evaluate: \n",
        "        plot_loss(\n",
        "            [train_loss, train_recon_loss],\n",
        "            [test_loss, test_recon_loss],\n",
        "        )\n",
        "        \n",
        "    return vae"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isT5TzCPJEE1"
      },
      "source": [
        "def eval_model(vae):\n",
        "    vae.eval()\n",
        "    \n",
        "    test_loss_avg, test_recon_loss_avg, num_batches = 0, 0, 0\n",
        "    sum = 0\n",
        "    for image_batch, _ in test_dataloader:\n",
        "        \n",
        "        with torch.no_grad():\n",
        "        \n",
        "            image_batch = image_batch.to(device)\n",
        "    \n",
        "            # vae reconstruction\n",
        "            image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "    \n",
        "            # reconstruction error\n",
        "            recon_loss = reconstruction_error(image_batch_recon, image_batch)\n",
        "            loss = vae_loss(recon_loss, latent_mu, latent_logvar)\n",
        "    \n",
        "            test_recon_loss_avg += recon_loss\n",
        "            test_loss_avg += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "    test_recon_loss_avg /= num_batches\n",
        "    test_loss_avg /= num_batches\n",
        "\n",
        "    return test_recon_loss_avg, test_loss_avg\n",
        "    #print('average reconstruction error: %f' % (test_recon_loss_avg))\n",
        "    #print('average error: %f' % (test_loss_avg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idP_RI6bBttt"
      },
      "source": [
        "def train_and_test_models(\n",
        "    n, gamma_matrix, alpha_matrix, beta_matrix, evaluate = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train n models and compute the MNIST heterogeneities on the models.\n",
        "\n",
        "    Returns:\n",
        "        Heterogeneity \"matrices\" that are lists of np arrrays.\n",
        "        matrix[i] is the heterogeneity array for vae_i\n",
        "        matrix[i][j] is the heterogeneity for digit_j for vae_i\n",
        "    \"\"\"\n",
        "    for _ in range(n):\n",
        "        vae = train_one_model(evaluate = evaluate)\n",
        "        \n",
        "        gammas, alphas, betas = calculate_rrh(vae)\n",
        "        gamma_matrix.append(gammas)\n",
        "        alpha_matrix.append(alphas)\n",
        "        beta_matrix.append(betas)\n",
        "    return gamma_matrix, alpha_matrix, beta_matrix\n",
        "\n",
        "def plot_rrh_matrices(gamma_matrix, alpha_matrix, beta_matrix):\n",
        "    gamma_avg = het_avg(gamma_matrix)\n",
        "    alpha_avg = het_avg(alpha_matrix)\n",
        "    beta_avg = het_avg(beta_matrix)\n",
        "    \n",
        "    gamma_sigma = het_sigma(gamma_matrix)\n",
        "    alpha_sigma = het_sigma(alpha_matrix)\n",
        "    beta_sigma = het_sigma(beta_matrix)\n",
        "    \n",
        "    plot_rrh(\n",
        "        gamma_avg, alpha_avg, beta_avg,\n",
        "        sigmas = [gamma_sigma, alpha_sigma, beta_sigma],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxImy1XR6FHR"
      },
      "source": [
        "## RRH for Gaussian Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H2L9XvS6Kwj"
      },
      "source": [
        "def mvn_renyi(C, q=1):\n",
        "    \"\"\" Computes the Rnyi heterogeneity for a multivariate Gaussian \n",
        "    Arguments: \n",
        "        C: `ndarray((n,n))`. Covariance matrix\n",
        "        q: `0<float`. Order of the heterogeneity\n",
        "    Returns: \n",
        "        `float`\n",
        "    \"\"\"\n",
        "    n = C.shape[0]\n",
        "    SqrtDetC = np.sqrt(np.linalg.det(C))\n",
        "    if q == 1: \n",
        "        out = (2*np.pi*np.e)**(n/2) * SqrtDetC\n",
        "    elif q == np.inf: \n",
        "        out = (2*np.pi)**(n/2) * SqrtDetC\n",
        "    elif q!=1 and q!=0 and q!=np.inf:\n",
        "        out = ((2*np.pi)**(n/2))*(q**(n/(2*(q-1))))*SqrtDetC\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkLYVMBu6fDn"
      },
      "source": [
        "def mvn_renyi_alpha(C,  q=1):\n",
        "    \"\"\" Computes the alpha-heterogeneity for a Gaussian mixture where each sample has equal weight\n",
        "\n",
        "    Arguments: \n",
        "\n",
        "        cov: `ndarray((nsamples, n, n))`. Covariance matrices \n",
        "        q: `0<float`. Order of the heterogeneity metric\n",
        "\n",
        "    Returns: \n",
        "\n",
        "        `float`. The alpha-heterogeneity\n",
        "    \"\"\"\n",
        "    K, n, _ = C.shape\n",
        "    p = np.repeat(1/K, K)\n",
        "    if q == 1:\n",
        "        out = np.exp((n + np.sum(p*np.log(np.linalg.det(2*np.pi*C))))/2)\n",
        "    elif q!=np.inf and q!=1 and q!=0:\n",
        "        wbar = (p**q)/np.sum(p**q)\n",
        "        out = ((2*np.pi)**(n/2))*np.sum(wbar*np.sqrt(np.linalg.det(C)))/(q**(n/2))**(1/(1-q))\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aDzvMEx_X9r"
      },
      "source": [
        "def scale_to_cov(scales):\n",
        "    return np.vstack([np.expand_dims(np.diagflat(s), 0) for s in scales])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS7dTSHl_e2i"
      },
      "source": [
        "def pool_covariance(means, covs):\n",
        "    K = covs.shape[0] \n",
        "    p = np.repeat(1/K, K)\n",
        "    cov_ = np.einsum('ijk,i->jk', covs, p) + np.einsum('ij,ik,i->jk', means, means, p)\n",
        "    mu_ = np.einsum('ij,i->j', means, p)\n",
        "    return cov_ - np.einsum('i,j->ij', mu_, mu_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG67lA7u6pAY"
      },
      "source": [
        "### Computation and plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IinU-xeM6fg_"
      },
      "source": [
        "def calculate_rrh(vae, X = train_X, y = train_y):\n",
        "    gammas, alphas, betas = [], [], []\n",
        "    for i in range(10):\n",
        "        mu, logvar = vae.encoder(torch.Tensor(X[y == i]).to(device))\n",
        "        loc = mu.cpu().detach().numpy()\n",
        "        scale = logvar.exp().cpu().detach().numpy()\n",
        "        cov = scale_to_cov(scale)\n",
        "        gamma = mvn_renyi(pool_covariance(loc, cov), q=1)\n",
        "        alpha = mvn_renyi_alpha(cov,q=1)\n",
        "        beta = gamma/alpha\n",
        "        gammas.append(gamma)\n",
        "        alphas.append(alpha)\n",
        "        betas.append(beta)\n",
        "    return np.array(gammas), np.array(alphas), np.array(betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcRWRha365r-"
      },
      "source": [
        "def plot_rrh(gammas, alphas, betas, sigmas = None):\n",
        "    if not (len(gammas) == len(alphas) or len(gammas) == len(betas)):\n",
        "        sys.exit(\"Mismatched matrix size\")\n",
        "    n = len(gammas)\n",
        "    hetvalues = [gammas, alphas, betas]\n",
        "    plotlabels = [r\"Pooled\", r\"Within-Observation\", r\"Between-Observation\"]\n",
        "    \n",
        "    fig, ax = plt.subplots(ncols=3, figsize=(9, 2.5))\n",
        "    ax[0].set_ylabel(\"Heterogeneity\")\n",
        "    \n",
        "    for i in range(3): \n",
        "        ax[i].set_title(plotlabels[i])\n",
        "        ax[i].set_xlabel(\"Digit\")\n",
        "        ax[i].set_xticks(np.arange(10))\n",
        "        ax[i].set_xticklabels(np.arange(10))\n",
        "        ax[i].bar(\n",
        "            np.arange(10),\n",
        "            hetvalues[i],\n",
        "            facecolor = plt.get_cmap(\"Greys\")(0.4), \n",
        "            edgecolor = \"black\",\n",
        "        )\n",
        "        if not sigmas == None:\n",
        "            ax[i].errorbar(\n",
        "                np.arange(10),\n",
        "                hetvalues[i],\n",
        "                yerr = sigmas[i],\n",
        "                fmt = \"none\",\n",
        "                ecolor = \"r\",\n",
        "                capsize = 3,\n",
        "                label = \"std deviation\",\n",
        "            )\n",
        "            ax[i].errorbar(\n",
        "                np.arange(10),\n",
        "                hetvalues[i],\n",
        "                yerr = sigmas[i] / np.sqrt(n),\n",
        "                fmt = \"none\",\n",
        "                ecolor = \"b\",\n",
        "                capsize = 3,\n",
        "                label = \"std error\",\n",
        "            )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    #plt.savefig(\"digit-class-heterogeneity.pdf\", bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOOtu98Jv1-_"
      },
      "source": [
        "def het_sigma(matrix, avg = None):\n",
        "    if avg == None:\n",
        "        avg = het_avg(matrix)\n",
        "    n = len(matrix)\n",
        "    mse = (matrix[0] - avg) ** 2\n",
        "    for i in range(1, n):\n",
        "        mse += (matrix[i] - avg) ** 2\n",
        "    return np.sqrt(mse / n)\n",
        "\n",
        "def het_sum(matrix):\n",
        "    sum = matrix[0] + matrix[1]\n",
        "    for i in range(2, len(matrix)):\n",
        "        sum += matrix[i]\n",
        "    return sum\n",
        "\n",
        "def het_avg(matrix):\n",
        "    return het_sum(matrix) / len(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM3hF-q0VipP"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzmQvee5qsLp"
      },
      "source": [
        "\"\"\"\n",
        "N = 1\n",
        "n = 5\n",
        "gamma_matrix, alpha_matrix, beta_matrix = [], [], []\n",
        "for _ in range(N):\n",
        "    gamma_matrix, alpha_matrix, beta_matrix = train_and_test_models(\n",
        "        n, gamma_matrix, alpha_matrix, beta_matrix, evaluate = True,\n",
        "    )\n",
        "    plot_rrh_matrices(gamma_matrix, alpha_matrix, beta_matrix)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvx8i8Ct9yMR"
      },
      "source": [
        "## Load, Evaluate, and Plot RRH for Pre-Trained VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNRYGlvQ5O77"
      },
      "source": [
        "def load_pretrained_vae():\n",
        "    pretrained_vae = VariationalAutoencoder()\n",
        "    pretrained_vae = pretrained_vae.to(device)\n",
        "    \n",
        "    filename = 'vae_2d.pth'\n",
        "    \n",
        "    if not os.path.isdir('./pretrained'):\n",
        "        os.makedirs('./pretrained')\n",
        "    urllib.request.urlretrieve(\n",
        "        \"http://geometry.cs.ucl.ac.uk/creativeai/pretrained/\" + filename,\n",
        "        \"./pretrained/\" + filename,\n",
        "    )\n",
        "    pretrained_vae.load_state_dict(torch.load('./pretrained/' + filename))\n",
        "    return pretrained_vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRS4NG0mJJZY"
      },
      "source": [
        "pretrained_vae = load_pretrained_vae()\n",
        "eval_model(pretrained_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nN1Kftyh42Z"
      },
      "source": [
        "gammas, alphas, betas = calculate_rrh(pretrained_vae)\n",
        "plot_rrh(gammas, alphas, betas)\n",
        "\n",
        "gammas, alphas, betas = calculate_rrh(pretrained_vae, X = test_X, y = test_y)\n",
        "plot_rrh(gammas, alphas, betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzodKmcX0SLi"
      },
      "source": [
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualize_latent_space(vae):\n",
        "    vae.eval()\n",
        "\n",
        "    bound = 2.5\n",
        "    nsteps = 20 \n",
        "    with torch.no_grad():\n",
        "        latent_x = np.linspace(-bound,bound,nsteps)\n",
        "        latent_y = np.linspace(-bound,bound,nsteps)\n",
        "        latents = torch.FloatTensor(len(latent_x), len(latent_y), 2)\n",
        "        for i, lx in enumerate(latent_x):\n",
        "            for j, ly in enumerate(latent_y):\n",
        "                latents[j, i, 0] = lx\n",
        "                latents[j, i, 1] = ly\n",
        "        latents = latents.view(-1, 2) # flatten grid into a batch\n",
        "    \n",
        "        # reconstruct images from the latent vectors\n",
        "        latents = latents.to(device)\n",
        "        image_recon = vae.decoder(latents)\n",
        "        image_recon = image_recon.cpu()\n",
        "    \n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        show_image(\n",
        "            torchvision.utils.make_grid(\n",
        "                image_recon.data[:int(nsteps**2)], nsteps, 5,\n",
        "            )\n",
        "        )\n",
        "        ax.set_xticks([]); ax.set_xticklabels([])\n",
        "        ax.set_yticks([]); ax.set_yticklabels([])\n",
        "        #plt.savefig(\"latent-space-map.pdf\", bbox_inches=\"tight\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v0TSx231Las"
      },
      "source": [
        "visualize_latent_space(vae1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_NVuvet7_e4"
      },
      "source": [
        "visualize_latent_space(vae2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsksiYr20gME"
      },
      "source": [
        "visualize_latent_space(pretrained_vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa8BsW-sxvOf"
      },
      "source": [
        "def visualise_output(images, model, title = \"DEFAULT\"):\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        images = images.to(device)\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[0:IMG_NUM], WIDTH, HEIGHT).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.title(title)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxYy03Fdtlwn"
      },
      "source": [
        "IMG_NUM = 100\n",
        "WIDTH = 10\n",
        "HEIGHT = 10\n",
        "\n",
        "sns.set(style = \"dark\")\n",
        "\n",
        "images, labels = iter(test_dataloader).next()\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[0:IMG_NUM], WIDTH, HEIGHT))\n",
        "plt.title(\"Original\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "visualise_output(images, vae1, title = \"VAE 1\")\n",
        "\n",
        "visualise_output(images, vae2, title = \"VAE 2\")\n",
        "\n",
        "visualise_output(images, pretrained_vae, title = \"Pretrained VAE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjWWbUCA0oKk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}